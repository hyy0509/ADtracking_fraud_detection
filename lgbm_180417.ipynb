{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "debug=0\n",
    "if debug:\n",
    "    print('*** debug parameter set: this is a test run for debugging purposes ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.2,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "\n",
    "    return (bst1,bst1.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DO(frm,to,fileno):\n",
    "    dtypes = {\n",
    "            'ip'            : 'uint32',\n",
    "            'app'           : 'uint16',\n",
    "            'device'        : 'uint16',\n",
    "            'os'            : 'uint16',\n",
    "            'channel'       : 'uint16',\n",
    "            'is_attributed' : 'uint8',\n",
    "            'click_id'      : 'uint32',\n",
    "            }\n",
    "\n",
    "    print('loading train data...')\n",
    "    train_df = pd.read_csv(\"../data/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "    print('loading test data...')\n",
    "    if debug:\n",
    "        test_df = pd.read_csv(\"../data/test.csv\", nrows=100000, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"../data/test.csv\", parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "    len_train = len(train_df)\n",
    "    train_df=train_df.append(test_df)\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Extracting new features...')\n",
    "    train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "    train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    naddfeat=9\n",
    "    for i in range(0,naddfeat):\n",
    "        if i==0: selcols=['ip', 'channel']; QQ=4;\n",
    "        if i==1: selcols=['ip', 'device', 'os', 'app']; QQ=5;\n",
    "        if i==2: selcols=['ip', 'day', 'hour']; QQ=4;\n",
    "        if i==3: selcols=['ip', 'app']; QQ=4;\n",
    "        if i==4: selcols=['ip', 'app', 'os']; QQ=4;\n",
    "        if i==5: selcols=['ip', 'device']; QQ=4;\n",
    "        if i==6: selcols=['app', 'channel']; QQ=4;\n",
    "        if i==7: selcols=['ip', 'os']; QQ=5;\n",
    "        if i==8: selcols=['ip', 'device', 'os', 'app']; QQ=4;\n",
    "        print('selcols',selcols,'QQ',QQ)\n",
    "        \n",
    "        filename='X%d_%d_%d.csv'%(i,frm,to)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            if QQ==5: \n",
    "                gp=pd.read_csv(filename,header=None)\n",
    "                train_df['X'+str(i)]=gp\n",
    "            else: \n",
    "                gp=pd.read_csv(filename)\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "        else:\n",
    "            if QQ==0:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].count().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==1:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].mean().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==2:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].var().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==3:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].skew().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==4:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].nunique().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==5:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].cumcount()\n",
    "                train_df['X'+str(i)]=gp.values\n",
    "            \n",
    "            if not debug:\n",
    "                 gp.to_csv(filename,index=False)\n",
    "            \n",
    "        del gp\n",
    "        gc.collect()    \n",
    "\n",
    "    print('doing nextClick')\n",
    "    predictors=[]\n",
    "    \n",
    "    new_feature = 'nextClick'\n",
    "    filename='nextClick_%d_%d.csv'%(frm,to)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print('loading from save file')\n",
    "        QQ=pd.read_csv(filename).values\n",
    "    else:\n",
    "        D=2**26\n",
    "        train_df['category'] = (train_df['ip'].astype(str) + \"_\" + train_df['app'].astype(str) + \"_\" + train_df['device'].astype(str) \\\n",
    "            + \"_\" + train_df['os'].astype(str)).apply(hash) % D\n",
    "        click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "        train_df['epochtime']= train_df['click_time'].astype(np.int64) // 10 ** 9\n",
    "        next_clicks= []\n",
    "        for category, t in zip(reversed(train_df['category'].values), reversed(train_df['epochtime'].values)):\n",
    "            next_clicks.append(click_buffer[category]-t)\n",
    "            click_buffer[category]= t\n",
    "        del(click_buffer)\n",
    "        QQ= list(reversed(next_clicks))\n",
    "\n",
    "        if not debug:\n",
    "            print('saving')\n",
    "            pd.DataFrame(QQ).to_csv(filename,index=False)\n",
    "\n",
    "    train_df[new_feature] = QQ\n",
    "    predictors.append(new_feature)\n",
    "\n",
    "    train_df[new_feature+'_shift'] = pd.DataFrame(QQ).shift(+1).values\n",
    "    predictors.append(new_feature+'_shift')\n",
    "    \n",
    "    del QQ\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-day-hour combination...')\n",
    "    gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_tcount'})\n",
    "    train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-app combination...')\n",
    "    gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-app-os combination...')\n",
    "    gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    # Adding features with var and mean hour (inspired from nuhsikander's script)\n",
    "    print('grouping by : ip_day_chl_var_hour')\n",
    "    gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','channel'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_tchan_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_os_var_hour')\n",
    "    gp = train_df[['ip','app', 'os', 'hour']].groupby(by=['ip', 'app', 'os'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_app_os_var'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_channel_var_day')\n",
    "    gp = train_df[['ip','app', 'channel', 'day']].groupby(by=['ip', 'app', 'channel'])[['day']].var().reset_index().rename(index=str, columns={'day': 'ip_app_channel_var_day'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_chl_mean_hour')\n",
    "    gp = train_df[['ip','app', 'channel','hour']].groupby(by=['ip', 'app', 'channel'])[['hour']].mean().reset_index().rename(index=str, columns={'hour': 'ip_app_channel_mean_hour'})\n",
    "    print(\"merging...\")\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"vars and data type: \")\n",
    "    train_df.info()\n",
    "    train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16')\n",
    "    train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "    train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')\n",
    "\n",
    "    target = 'is_attributed'\n",
    "    predictors.extend(['app','device','os', 'channel', 'hour', 'day', \n",
    "                  'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "                  'ip_app_os_count', 'ip_app_os_var',\n",
    "                  'ip_app_channel_var_day','ip_app_channel_mean_hour'])\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "    for i in range(0,naddfeat):\n",
    "        predictors.append('X'+str(i))\n",
    "        \n",
    "    print('predictors',predictors)\n",
    "\n",
    "    test_df = train_df[len_train:]\n",
    "    val_df = train_df[(len_train-val_size):len_train]\n",
    "    train_df = train_df[:(len_train-val_size)]\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "    print(\"test size : \", len(test_df))\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.20,\n",
    "        #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "        'num_leaves': 7,  # 2^max_depth - 1\n",
    "        'max_depth': 3,  # -1 means no limit\n",
    "        'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight':200 # because training data is extremely unbalanced \n",
    "    }\n",
    "    (bst,best_iteration) = lgb_modelfit_nocv(params, \n",
    "                            train_df, \n",
    "                            val_df, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            objective='binary', \n",
    "                            metrics='auc',\n",
    "                            early_stopping_rounds=30, \n",
    "                            verbose_eval=True, \n",
    "                            num_boost_round=1000, \n",
    "                            categorical_features=categorical)\n",
    "\n",
    "    print('[{}]: model training time'.format(time.time() - start_time))\n",
    "    del train_df\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "    \n",
    "    #print('Plot feature importances...')\n",
    "    #ax = lgb.plot_importance(bst, max_num_features=100)\n",
    "    #plt.show()\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    if not debug:\n",
    "        print(\"writing...\")\n",
    "        sub.to_csv('sub_lgbm_it%d.csv.gz'%(fileno),index=False,compression='gzip')\n",
    "    print(\"done...\")\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data... 109903890 149903890\n",
      "loading test data...\n",
      "Extracting new features...\n",
      "selcols ['ip', 'channel'] QQ 4\n",
      "selcols ['ip', 'device', 'os', 'app'] QQ 5\n",
      "selcols ['ip', 'day', 'hour'] QQ 4\n",
      "selcols ['ip', 'app'] QQ 4\n",
      "selcols ['ip', 'app', 'os'] QQ 4\n",
      "selcols ['ip', 'device'] QQ 4\n",
      "selcols ['app', 'channel'] QQ 4\n",
      "selcols ['ip', 'os'] QQ 5\n",
      "selcols ['ip', 'device', 'os', 'app'] QQ 4\n",
      "doing nextClick\n",
      "saving\n",
      "grouping by ip-day-hour combination...\n",
      "grouping by ip-app combination...\n",
      "grouping by ip-app-os combination...\n",
      "grouping by : ip_day_chl_var_hour\n",
      "grouping by : ip_app_os_var_hour\n",
      "grouping by : ip_app_channel_var_day\n",
      "grouping by : ip_app_chl_mean_hour\n",
      "merging...\n",
      "vars and data type: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58790469 entries, 0 to 58790468\n",
      "Data columns (total 30 columns):\n",
      "app                         uint16\n",
      "channel                     uint16\n",
      "click_id                    float64\n",
      "click_time                  datetime64[ns]\n",
      "device                      uint16\n",
      "ip                          uint32\n",
      "is_attributed               float64\n",
      "os                          uint16\n",
      "hour                        uint8\n",
      "day                         uint8\n",
      "X0                          int64\n",
      "X1                          int64\n",
      "X2                          int64\n",
      "X3                          int64\n",
      "X4                          int64\n",
      "X5                          int64\n",
      "X6                          int64\n",
      "X7                          int64\n",
      "X8                          int64\n",
      "category                    int64\n",
      "epochtime                   int64\n",
      "nextClick                   int64\n",
      "nextClick_shift             float64\n",
      "ip_tcount                   int64\n",
      "ip_app_count                int64\n",
      "ip_app_os_count             int64\n",
      "ip_tchan_count              float64\n",
      "ip_app_os_var               float64\n",
      "ip_app_channel_var_day      float64\n",
      "ip_app_channel_mean_hour    float64\n",
      "dtypes: datetime64[ns](1), float64(7), int64(15), uint16(4), uint32(1), uint8(2)\n",
      "memory usage: 11.3 GB\n",
      "predictors ['nextClick', 'nextClick_shift', 'app', 'device', 'os', 'channel', 'hour', 'day', 'ip_tcount', 'ip_tchan_count', 'ip_app_count', 'ip_app_os_count', 'ip_app_os_var', 'ip_app_channel_var_day', 'ip_app_channel_mean_hour', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
      "train size:  37500000\n",
      "valid size:  2500000\n",
      "test size :  18790469\n",
      "Training...\n",
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\ttrain's auc: 0.971499\tvalid's auc: 0.966822\n",
      "[20]\ttrain's auc: 0.977767\tvalid's auc: 0.972602\n",
      "[30]\ttrain's auc: 0.980211\tvalid's auc: 0.974423\n",
      "[40]\ttrain's auc: 0.981768\tvalid's auc: 0.975046\n",
      "[50]\ttrain's auc: 0.98269\tvalid's auc: 0.975883\n",
      "[60]\ttrain's auc: 0.983297\tvalid's auc: 0.976149\n",
      "[70]\ttrain's auc: 0.983632\tvalid's auc: 0.976376\n",
      "[80]\ttrain's auc: 0.984027\tvalid's auc: 0.976478\n",
      "[90]\ttrain's auc: 0.984281\tvalid's auc: 0.976619\n",
      "[100]\ttrain's auc: 0.984588\tvalid's auc: 0.976596\n",
      "[110]\ttrain's auc: 0.984798\tvalid's auc: 0.976593\n",
      "[120]\ttrain's auc: 0.984987\tvalid's auc: 0.976668\n",
      "[130]\ttrain's auc: 0.985161\tvalid's auc: 0.976745\n",
      "[140]\ttrain's auc: 0.985331\tvalid's auc: 0.976631\n",
      "[150]\ttrain's auc: 0.985455\tvalid's auc: 0.976717\n",
      "[160]\ttrain's auc: 0.985619\tvalid's auc: 0.976761\n",
      "[170]\ttrain's auc: 0.985723\tvalid's auc: 0.976762\n",
      "[180]\ttrain's auc: 0.985824\tvalid's auc: 0.976815\n",
      "[190]\ttrain's auc: 0.985912\tvalid's auc: 0.976807\n",
      "[200]\ttrain's auc: 0.986033\tvalid's auc: 0.976822\n",
      "[210]\ttrain's auc: 0.98613\tvalid's auc: 0.976861\n",
      "[220]\ttrain's auc: 0.986212\tvalid's auc: 0.976941\n",
      "[230]\ttrain's auc: 0.986281\tvalid's auc: 0.97693\n",
      "[240]\ttrain's auc: 0.986354\tvalid's auc: 0.976928\n",
      "[250]\ttrain's auc: 0.986446\tvalid's auc: 0.977012\n",
      "[260]\ttrain's auc: 0.986516\tvalid's auc: 0.977062\n",
      "[270]\ttrain's auc: 0.986598\tvalid's auc: 0.977088\n",
      "[280]\ttrain's auc: 0.986672\tvalid's auc: 0.977119\n",
      "[290]\ttrain's auc: 0.986729\tvalid's auc: 0.977198\n",
      "[300]\ttrain's auc: 0.986816\tvalid's auc: 0.977193\n",
      "[310]\ttrain's auc: 0.986862\tvalid's auc: 0.977197\n",
      "[320]\ttrain's auc: 0.986928\tvalid's auc: 0.977203\n",
      "[330]\ttrain's auc: 0.986982\tvalid's auc: 0.977235\n",
      "[340]\ttrain's auc: 0.987039\tvalid's auc: 0.977252\n",
      "[350]\ttrain's auc: 0.987102\tvalid's auc: 0.977284\n",
      "[360]\ttrain's auc: 0.987153\tvalid's auc: 0.977331\n",
      "[370]\ttrain's auc: 0.987207\tvalid's auc: 0.977367\n",
      "[380]\ttrain's auc: 0.987271\tvalid's auc: 0.977327\n",
      "[390]\ttrain's auc: 0.987308\tvalid's auc: 0.977307\n",
      "[400]\ttrain's auc: 0.987359\tvalid's auc: 0.977417\n",
      "[410]\ttrain's auc: 0.987401\tvalid's auc: 0.977388\n",
      "[420]\ttrain's auc: 0.987462\tvalid's auc: 0.977417\n",
      "[430]\ttrain's auc: 0.987507\tvalid's auc: 0.977382\n",
      "Early stopping, best iteration is:\n",
      "[401]\ttrain's auc: 0.98737\tvalid's auc: 0.977425\n",
      "\n",
      "Model Report\n",
      "bst1.best_iteration:  401\n",
      "auc: 0.9774245161053575\n",
      "[2670.3498668670654]: model training time\n",
      "Predicting...\n",
      "writing...\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "nrows=184903891-1\n",
    "nchunk=40000000\n",
    "val_size=2500000\n",
    "\n",
    "frm=nrows-75000000\n",
    "if debug:\n",
    "    frm=0\n",
    "    nchunk=100000\n",
    "    val_size=10000\n",
    "\n",
    "to=frm+nchunk\n",
    "\n",
    "sub=DO(frm,to,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
